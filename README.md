# UBP Teleoperation

## 📖 프로젝트 개요
UBP Teleoperation & Vision 프로젝트는 **착용형 슈트 Leader**와 **비전 기반 헤드 트래킹**을 통합하여,  
사람의 움직임을 로봇에 자연스럽게 전달하는 것을 목표로 합니다.  

이 시스템은 **Dynamixel 모터 슈트**를 통해 사람의 팔/몸 움직임을 캡처하고,  
**카메라 + YOLO 모델**을 이용하여 사람(또는 얼굴)의 위치를 추적하여 로봇의 머리(Head)를 제어합니다.  

즉, 사용자가 슈트를 입고 움직이면 → 로봇이 같은 동작을 수행하고,  
카메라가 관람자(사람)의 얼굴을 인식하면 → 로봇의 머리가 관람자(사람)의 얼굴을 트랙킹 하며 상호작용 합니다.  

---

## 🎯 주요 목표
- **착용형 슈트**: 다이나믹셀 기반 슈트를 통해 사용자의 동작 데이터를 수집
- **비전 기반 추적**: YOLOv8을 이용한 얼굴/사람 검출
- **로봇 제어 연동**: 수집된 데이터 → ROS2 토픽 → 로봇의 바디 및 헤드 제어

---

## 시스템 구성

### 1. Teleoperation Leader(슈트 제어)
- Dynamixel 모터(1~14번)를 이용한 착용형 슈트
- 슈트에서 읽은 모터 위치를 `/ubp/body/cmd` 토픽으로 퍼블리시
- 로봇의 바디 모터 제어 입력으로 사용

### 2. Vision (카메라 + 얼굴 인식)
- RealSense D435i 등 카메라 입력을 수신
- YOLOv8 모델을 이용해 얼굴/사람을 인식
- 화면 중심과의 오프셋을 계산하여 `/ubp/vision/target`으로 퍼블리시

### 3. Head Tracking (헤드 추적)
- Vision에서 제공한 타깃 각도를 받아
- 로봇의 Head(Yaw, Pitch)를 제어할 수 있는 `/ubp/head/cmd`로 변환

### 4. Dynamixel Operator (Follower)
- Head + Body Dynamixel 모터를 제어하는 메인 노드
- PID 기반 제어로 부드러운 동작 보장
- 최종적으로 `joint_states` 퍼블리시 → RViz/로봇 시스템 연동 가능

---

## 데이터 흐름
1. **슈트** → `/ubp/body/cmd` → 로봇 바디 모터 제어  
2. **카메라** → YOLO 감지 → `/ubp/vision/target`  
3. **헤드 트래킹** → `/ubp/head/cmd` → 로봇 Head 제어  
4. **Dynamixel Controller** → 실제 모터 구동 + `joint_states` 퍼블리시  

---

## 기대 효과
- **자연스러운 원격 제어**: 사용자가 로봇의 몸/팔을 직접 움직이는 듯한 직관적 제어
- **비전 기반 상호작용**: 로봇이 사람의 얼굴을 추적하여 인터랙션 가능
- **확장성**: Head + Body 구조에서 전신 제어로 확장 가능
- **추후 lerobot과 같은 Pysical AI 적용**: Pysical AI적용 가능
---

## 기술 스택
- ROS2 Humble / Jazzy
- Dynamixel SDK
- Ultralytics YOLOv8
- Python3 기반 ROS2 노드

---

## 요약
이 프로젝트는 **착용형 슈트(입력)**와 **비전 기반 얼굴 추적(인식)**을 통합하여,  
**사람의 움직임을 로봇에 실시간 반영하는 원격 제어 시스템**입니다.  

로봇을 단순 제어하는 수준을 넘어서, **사람-로봇 간 자연스러운 상호작용(HRI)**을 구현하는 것을 목표로 합니다.
